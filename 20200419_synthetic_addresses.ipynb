{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Addresses with GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this notebook is to see, how hard it would be to generate synthetic addresses from existing addresses with the help of a GAN. Basis will be the sourced addresses from OpenAddresses Portugal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input, Flatten, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_address = pd.read_csv('./data/openaddr-collected-europe/pt/countrywide.csv').sample(frac=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886671, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_address.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This df is way too large. It will have to be downsampled to be useful. As it is I just don't have enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 886671 entries, 5420398 to 2293475\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   LON       886671 non-null  float64\n",
      " 1   LAT       886671 non-null  float64\n",
      " 2   NUMBER    661563 non-null  object \n",
      " 3   STREET    851397 non-null  object \n",
      " 4   UNIT      0 non-null       float64\n",
      " 5   CITY      885473 non-null  object \n",
      " 6   DISTRICT  0 non-null       float64\n",
      " 7   REGION    0 non-null       float64\n",
      " 8   POSTCODE  886671 non-null  object \n",
      " 9   ID        886671 non-null  object \n",
      " 10  HASH      886671 non-null  object \n",
      "dtypes: float64(5), object(6)\n",
      "memory usage: 81.2+ MB\n"
     ]
    }
   ],
   "source": [
    "open_address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>NUMBER</th>\n",
       "      <th>STREET</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>CITY</th>\n",
       "      <th>DISTRICT</th>\n",
       "      <th>REGION</th>\n",
       "      <th>POSTCODE</th>\n",
       "      <th>ID</th>\n",
       "      <th>HASH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5212010</th>\n",
       "      <td>-8.49116</td>\n",
       "      <td>41.783872</td>\n",
       "      <td>46</td>\n",
       "      <td>R POMBAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GANDRA PTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4990-640</td>\n",
       "      <td>pt.ine.add.PTCONT.5207942</td>\n",
       "      <td>1d4c6fcb006cdc1e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LON        LAT NUMBER    STREET  UNIT        CITY  DISTRICT  \\\n",
       "5212010 -8.49116  41.783872     46  R POMBAL   NaN  GANDRA PTL       NaN   \n",
       "\n",
       "         REGION  POSTCODE                         ID              HASH  \n",
       "5212010     NaN  4990-640  pt.ine.add.PTCONT.5207942  1d4c6fcb006cdc1e  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_address.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886671, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_address.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the only relevant information is the address, from which we will create sequences of Booleans. The final input shape will therefore be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(number_of_samples x address_length x number_of_distinct_characters) Whereby the address_length will be the length of the longest address in characters. Shorter addresses will have fillers appended to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of characters, all will be eligible, although, all strings will be converted to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_address.fillna('', inplace=True)\n",
    "construct_mat = open_address[['STREET', 'NUMBER', 'POSTCODE', 'CITY']].values\n",
    "address = map(lambda x: f'{x[0]} {x[1]}, {x[2]} {x[3]}'.replace('  ', ' ').replace(' ,', ',').strip().title(), tqdm(construct_mat))\n",
    "addresses = tuple(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(addresses, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join(addresses)\n",
    "text = tuple(set(text))\n",
    "\n",
    "encoding = {letter: i for i, letter in enumerate(text)}\n",
    "decoding = {i: letter for i, letter in enumerate(text)}\n",
    "\n",
    "y = np.zeros((len(addresses), maxlen, len(encoding)), dtype=bool)\n",
    "for i, address in tqdm(enumerate(addresses)):\n",
    "    for t, letter in enumerate(address):\n",
    "        y[i, t, encoding[letter]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_generator():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(maxlen, 90)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(256)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(len(encoding), activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = _make_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     X,\n",
    "#     y,\n",
    "#     epochs=3,\n",
    "#     batch_size=128,\n",
    "#     validation_split=0.15,\n",
    "#     shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.zeros((len(addresses), maxlen), dtype=np.int64)\n",
    "# for i in range(len(addresses)):\n",
    "#     for t in range(X_test.shape[1]):\n",
    "#         new = np.random.randint(10**10, 9*10**10)\n",
    "#         X_test[i, t] = new\n",
    "        \n",
    "# X_test = X_test.reshape(len(addresses), maxlen, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_prediction(matrix_row):\n",
    "#     decoded = map(lambda unit: decoding[np.argmax(unit)], matrix_row)\n",
    "    \n",
    "#     return '.'.join(list(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = decode_prediction(y_hat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the preliminary results of the generator are not really what expected. My hypothesis is, that a generator was never meant to be that way. Instead of an actual generatr, what I build was the second side of a variational autoencoder. \n",
    "\n",
    "The next attempt will see to creating the generator and then then the generator will ONLY be trained at fooling the discriminator. I think now it is trying to learn the relationship between random noise and addresses, which does not exists. With the discriminator, it will learn how to manipulate random noise in a way that fools the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True), input_shape=(maxlen, len(encoding))))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(256)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(lr=0.1)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = _make_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan(discriminator, generator):\n",
    "    discriminator.trainable=False\n",
    "    gan_input = Input(shape=(maxlen, 90))\n",
    "    x = generator(gan_input)\n",
    "    gan_output= discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return gan\n",
    "gan = create_gan(discriminator,generator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the most recent results, I am pretty sure I will have to enable the discriminator to analyse sequences, instead of singular vectors (duh...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "generator= _make_generator()\n",
    "discriminator= _make_discriminator()\n",
    "gan = create_gan(discriminator, generator)\n",
    "losses = []\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    print(f'Epoch: {e}')\n",
    "    for _ in tqdm(range(batch_size)):\n",
    "        noise = np.random.normal(0,1, [batch_size, maxlen, 90])\n",
    "        fake_stuff = generator.predict(noise)\n",
    "        real_addresses = y[np.random.randint(low=0,high=y.shape[0],size=batch_size)]\n",
    "        X = np.concatenate([real_addresses, fake_stuff])\n",
    "        y_dis=np.zeros(2*batch_size, dtype=bool)\n",
    "        y_dis[:batch_size] = 1\n",
    "        discriminator.trainable=True\n",
    "        discriminator.train_on_batch(X, y_dis)\n",
    "        noise = np.random.normal(0,1, [batch_size, maxlen, 90])\n",
    "        y_gen = np.ones(batch_size)\n",
    "        discriminator.trainable=False\n",
    "        acc_batch_loss = gan.train_on_batch(noise, y_gen, reset_metrics=False)\n",
    "        losses.append(acc_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noise = np.random.normal(0,1, [500, maxlen, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake_stuff = generator.predict(test_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake_stuff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_synths(synth):\n",
    "    decoded_rows = []\n",
    "    for row in tqdm(synth):\n",
    "        decoded_row = map(lambda x: decoding[np.argmax(x)], row)\n",
    "        decoded_rows.append(''.join(tuple(decoded_row)))\n",
    "        \n",
    "    return decoded_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = decode_synths(test_fake_stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "generator.save('saved_model/generator')\n",
    "discriminator.save('saved_model/discriminator')\n",
    "gan.save('saved_model/gan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the results are garbage. I have now some theories why they are like that:\n",
    "- Batch size is to large. Lower batch size might increase diversity.\n",
    "- Base data is too small. Larger data set might alleviate this issue.\n",
    "- Something with the batch processing. If nothing else helps, I would like to try to train the models directly instead of in a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Results:\n",
    "- Until now, I could not really test any new approaches, since the model just really quickly gets too big for memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
